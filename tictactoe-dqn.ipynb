{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use cuda device\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from kaggle_environments import make\n",
    "\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "device = T.device('cuda' if T.cuda.is_available() else 'cpu')\n",
    "print(f'Use {device} device')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = '.'\n",
    "TRAP_STATE = np.ones((3, 3), dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QMemory:\n",
    "    def __init__(self, max_size, dims):\n",
    "        self.count = 0\n",
    "        self.max_size = max_size\n",
    "        \n",
    "        self.dones = np.zeros(max_size)\n",
    "        self.actions = np.zeros(max_size)\n",
    "        self.rewards = np.zeros(max_size)\n",
    "\n",
    "        self.prev_states = np.zeros((max_size, *dims))\n",
    "        self.next_states = np.zeros((max_size, *dims))\n",
    "    \n",
    "    def add(self, prev_state, action, next_state, reward, done):\n",
    "        k = self.count % self.max_size\n",
    "        self.count += 1\n",
    "        \n",
    "        self.dones[k] = done\n",
    "        self.rewards[k] = reward\n",
    "        self.actions[k] = action\n",
    "        self.prev_states[k] = prev_state\n",
    "        self.next_states[k] = next_state\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        siz = min(self.count, batch_size)\n",
    "        idx = np.random.choice(siz, batch_size, replace=False)\n",
    "        return self.prev_states[idx], self.actions[idx], self.next_states[idx], self.rewards[idx], self.dones[idx]  \n",
    "    \n",
    "class QEpsilon:\n",
    "    def __init__(self, val=1, min=1e-4, dec=1e-5):\n",
    "        self.val = val\n",
    "        self.min = min\n",
    "        self.dec = dec\n",
    "\n",
    "    def decrease(self):\n",
    "        self.val = max(self.val - self.dec, self.min)\n",
    "        \n",
    "    @property\n",
    "    def value(self):\n",
    "        return self.val\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent:\n",
    "    def __init__(self, dims=(3,3), n_actions=9, gamma=0.99, batch_size=64, mem_size=512, device=device, replace_rate=1000, eps=None):\n",
    "\n",
    "        if eps is None:\n",
    "            eps = QEpsilon(1, 1e-3, 1e-4)\n",
    "        elif eps == 0:\n",
    "            eps = QEpsilon(0, 0, 0)            \n",
    "            \n",
    "        self.device = device\n",
    "\n",
    "        self.memory = QMemory(mem_size, dims)\n",
    "        self.q_eval = self.create_q(dims, n_actions, device, True)\n",
    "        self.q_next = self.create_q(dims, n_actions, device, False)\n",
    "        self.action = np.arange(n_actions)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.loss = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.q_eval.parameters())\n",
    "        \n",
    "        self.batch_count = 0\n",
    "        self.replace_rate = replace_rate\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_q(dims, n_actions, device, is_eval):\n",
    "        q = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 2),\n",
    "            nn.Conv2d(64, 256, 2),\n",
    "            nn.Flatten(),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(256, n_actions),\n",
    "        ).to(device)\n",
    "        if is_eval:\n",
    "            q.eval()\n",
    "        else:\n",
    "            q.train()\n",
    "        return q\n",
    "    \n",
    "    def save(self):\n",
    "        T.save(self.q_eval.state_dict(), f'{CACHE_DIR}/eval.q')\n",
    "        T.save(self.q_next.state_dict(), f'{CACHE_DIR}/next.q')\n",
    "        \n",
    "    def load(self):\n",
    "        self.q_eval.load_state_dict(T.load(f'{CACHE_DIR}/eval.q'))\n",
    "        self.q_next.load_state_dict(T.load(f'{CACHE_DIR}/next.q'))\n",
    "        \n",
    "    def tensor(self, array, dtype=T.float):\n",
    "        return T.tensor(array, dtype=dtype, device=self.device)\n",
    "    \n",
    "    def __call__(self, board):\n",
    "        if np.random.rand() <= self.eps.value:            \n",
    "            action = np.random.choice(self.action)\n",
    "        else:\n",
    "            action = self.q_eval(self.tensor(board.reshape(1, 1, 3, 3))).argmax().item()\n",
    "        return int(action)\n",
    "    \n",
    "    def learn(self, prev_state, action, next_state, reward, done):\n",
    "        \n",
    "        self.memory.add(prev_state, action, next_state, reward, done)\n",
    "        if self.memory.count < self.batch_size:\n",
    "            return\n",
    "\n",
    "        \n",
    "        self.batch_count += 1\n",
    "        if self.batch_count >= self.replace_rate:\n",
    "            self.batch_count = 0\n",
    "            self.q_next.load_state_dict(self.q_eval.state_dict())\n",
    "            \n",
    "        prev_states, actions, next_states, rewards, dones = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        batch_idx = np.arange(self.batch_size)\n",
    "        q_pred = self.q_eval(self.tensor(prev_states.reshape(-1, 1, 3, 3)))[batch_idx, actions]\n",
    "        q_next = self.q_next(self.tensor(next_states.reshape(-1, 1, 3, 3))).max(dim=1)[0]\n",
    "        \n",
    "        q_next[dones] = 0.0\n",
    "        q_targ = self.tensor(rewards) + self.gamma * q_next\n",
    "        \n",
    "        loss = self.loss(q_pred.flatten(), q_targ.flatten())\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.eps.decrease()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTrainer:\n",
    "    def __init__(self, env, other=\"random\"):\n",
    "        self.trainer = env.train([None, other])\n",
    "\n",
    "    @staticmethod\n",
    "    def to_state(obs):\n",
    "        board = np.array(obs['board']).reshape(3,3)\n",
    "        board[board == 2] = -1\n",
    "        return board\n",
    "    \n",
    "    def reset(self):\n",
    "        return self.to_state(self.trainer.reset())\n",
    "\n",
    "    def step(self, action):\n",
    "        state, reward, done, info = self.trainer.step(int(action))\n",
    "        state = self.to_state(state)\n",
    "        if done:\n",
    "            state = TRAP_STATE\n",
    "        return state, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 1000. Wins: 80, Loose: 71, Draw: 2189, Invalid Moves: 848, Eps: 0.6875000000000344\n",
      "Game 2000. Wins: 99, Loose: 62, Draw: 2123, Invalid Moves: 837, Eps: 0.3754000000000688\n",
      "Game 3000. Wins: 91, Loose: 25, Draw: 1988, Invalid Moves: 884, Eps: 0.07660000000009498\n",
      "Game 4000. Wins: 132, Loose: 30, Draw: 1924, Invalid Moves: 836, Eps: 0.001\n",
      "Game 5000. Wins: 141, Loose: 44, Draw: 2275, Invalid Moves: 815, Eps: 0.001\n",
      "Game 6000. Wins: 702, Loose: 5, Draw: 2034, Invalid Moves: 293, Eps: 0.001\n",
      "Game 7000. Wins: 786, Loose: 6, Draw: 2026, Invalid Moves: 208, Eps: 0.001\n",
      "Game 8000. Wins: 722, Loose: 26, Draw: 2083, Invalid Moves: 252, Eps: 0.001\n",
      "Game 9000. Wins: 400, Loose: 47, Draw: 2004, Invalid Moves: 553, Eps: 0.001\n",
      "Game 10000. Wins: 882, Loose: 12, Draw: 2124, Invalid Moves: 106, Eps: 0.001\n",
      "Game 11000. Wins: 826, Loose: 16, Draw: 2098, Invalid Moves: 158, Eps: 0.001\n",
      "Game 12000. Wins: 823, Loose: 25, Draw: 2260, Invalid Moves: 152, Eps: 0.001\n",
      "Game 13000. Wins: 768, Loose: 20, Draw: 2197, Invalid Moves: 212, Eps: 0.001\n",
      "Game 14000. Wins: 779, Loose: 8, Draw: 2146, Invalid Moves: 213, Eps: 0.001\n",
      "Game 15000. Wins: 816, Loose: 8, Draw: 2111, Invalid Moves: 176, Eps: 0.001\n",
      "Game 16000. Wins: 827, Loose: 7, Draw: 2127, Invalid Moves: 166, Eps: 0.001\n",
      "Game 17000. Wins: 868, Loose: 13, Draw: 2162, Invalid Moves: 119, Eps: 0.001\n",
      "Game 18000. Wins: 886, Loose: 11, Draw: 2136, Invalid Moves: 103, Eps: 0.001\n",
      "Game 19000. Wins: 865, Loose: 13, Draw: 2220, Invalid Moves: 122, Eps: 0.001\n",
      "Game 20000. Wins: 865, Loose: 10, Draw: 2133, Invalid Moves: 125, Eps: 0.001\n",
      "Game 21000. Wins: 890, Loose: 10, Draw: 2105, Invalid Moves: 100, Eps: 0.001\n",
      "Game 22000. Wins: 882, Loose: 22, Draw: 2149, Invalid Moves: 96, Eps: 0.001\n",
      "Game 23000. Wins: 861, Loose: 14, Draw: 2125, Invalid Moves: 125, Eps: 0.001\n",
      "Game 24000. Wins: 866, Loose: 8, Draw: 2141, Invalid Moves: 126, Eps: 0.001\n",
      "Game 25000. Wins: 895, Loose: 9, Draw: 2118, Invalid Moves: 96, Eps: 0.001\n",
      "Game 26000. Wins: 865, Loose: 14, Draw: 2081, Invalid Moves: 121, Eps: 0.001\n",
      "Game 27000. Wins: 802, Loose: 8, Draw: 2205, Invalid Moves: 190, Eps: 0.001\n",
      "Game 28000. Wins: 752, Loose: 8, Draw: 2162, Invalid Moves: 240, Eps: 0.001\n",
      "Game 29000. Wins: 807, Loose: 8, Draw: 2189, Invalid Moves: 185, Eps: 0.001\n",
      "Game 30000. Wins: 881, Loose: 7, Draw: 2151, Invalid Moves: 112, Eps: 0.001\n",
      "Game 31000. Wins: 839, Loose: 19, Draw: 2216, Invalid Moves: 142, Eps: 0.001\n",
      "Game 32000. Wins: 813, Loose: 12, Draw: 2198, Invalid Moves: 175, Eps: 0.001\n",
      "Game 33000. Wins: 867, Loose: 7, Draw: 2149, Invalid Moves: 126, Eps: 0.001\n",
      "Game 34000. Wins: 895, Loose: 6, Draw: 2166, Invalid Moves: 99, Eps: 0.001\n",
      "Game 35000. Wins: 829, Loose: 20, Draw: 2218, Invalid Moves: 151, Eps: 0.001\n",
      "Game 36000. Wins: 889, Loose: 10, Draw: 2195, Invalid Moves: 101, Eps: 0.001\n",
      "Game 37000. Wins: 874, Loose: 4, Draw: 2167, Invalid Moves: 122, Eps: 0.001\n",
      "Game 38000. Wins: 893, Loose: 11, Draw: 2156, Invalid Moves: 96, Eps: 0.001\n",
      "Game 39000. Wins: 892, Loose: 16, Draw: 2162, Invalid Moves: 92, Eps: 0.001\n",
      "Game 40000. Wins: 867, Loose: 9, Draw: 2128, Invalid Moves: 124, Eps: 0.001\n",
      "Game 41000. Wins: 870, Loose: 15, Draw: 2191, Invalid Moves: 115, Eps: 0.001\n",
      "Game 42000. Wins: 903, Loose: 5, Draw: 2166, Invalid Moves: 92, Eps: 0.001\n",
      "Game 43000. Wins: 901, Loose: 4, Draw: 2158, Invalid Moves: 95, Eps: 0.001\n",
      "Game 44000. Wins: 890, Loose: 2, Draw: 2097, Invalid Moves: 108, Eps: 0.001\n",
      "Game 45000. Wins: 846, Loose: 8, Draw: 2242, Invalid Moves: 146, Eps: 0.001\n",
      "Game 46000. Wins: 883, Loose: 8, Draw: 2091, Invalid Moves: 109, Eps: 0.001\n",
      "Game 47000. Wins: 844, Loose: 11, Draw: 2082, Invalid Moves: 145, Eps: 0.001\n",
      "Game 48000. Wins: 879, Loose: 0, Draw: 2126, Invalid Moves: 121, Eps: 0.001\n",
      "Game 49000. Wins: 888, Loose: 10, Draw: 2084, Invalid Moves: 102, Eps: 0.001\n",
      "Game 50000. Wins: 859, Loose: 10, Draw: 2147, Invalid Moves: 131, Eps: 0.001\n"
     ]
    }
   ],
   "source": [
    "n_games = 50000\n",
    "\n",
    "env = make('tictactoe')\n",
    "trainer = QTrainer(env)\n",
    "agent = QAgent(batch_size=64, mem_size=1024, replace_rate=200)\n",
    "\n",
    "wins, loose, draw, invalid = 0, 0, 0, 0\n",
    "\n",
    "for game_no in range(n_games):\n",
    "    done = False\n",
    "    prev_state = trainer.reset()\n",
    "    while not done: \n",
    "        action = agent(prev_state)\n",
    "        next_state, reward, done = trainer.step(action)\n",
    "        \n",
    "        if reward == 1:\n",
    "            wins += 1\n",
    "        elif reward == -1:\n",
    "            loose += 1\n",
    "        elif reward == 0:\n",
    "            draw += 1\n",
    "        else:\n",
    "            invalid += 1\n",
    "            reward = -1\n",
    "\n",
    "        agent.learn(prev_state, action, next_state, reward, done)\n",
    "        prev_state = next_state\n",
    "\n",
    "    if game_no % 1000 == 999:\n",
    "        print(f'Game {game_no + 1}. Wins: {wins}, Loose: {loose}, Draw: {draw}, Invalid Moves: {invalid}, Eps: {agent.eps.value}')\n",
    "        wins, loose, draw, invalid = (0, 0, 0, 0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QClassicAgent:\n",
    "    def __init__(self, alpha=0.1, gamma=0.9):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.Q = np.random.rand(3 ** 9, 9)\n",
    "        self.eps = QEpsilon(1, 0, 1e-4)\n",
    "\n",
    "        self.Q[self.to_state(TRAP_STATE), :] = 0\n",
    "        \n",
    "    @staticmethod\n",
    "    def to_state(board):\n",
    "        return ((3 ** np.arange(9)) * (board.flatten() + 1)).sum()\n",
    "    \n",
    "    def __call__(self, board):\n",
    "        if np.random.rand() < self.eps.value:\n",
    "            return np.random.randint(9)\n",
    "        s = self.to_state(board)\n",
    "        a = self.Q[s, :].argmax()\n",
    "        return int(a)\n",
    "    \n",
    "    def learn(self, prev_state, action, next_state, reward, done):\n",
    "        a0 = action\n",
    "        if done:\n",
    "            next_state = TRAP_STATE\n",
    "            \n",
    "        s0 = self.to_state(prev_state)\n",
    "        s1 = self.to_state(next_state)\n",
    "        \n",
    "            \n",
    "        q_upd = reward + self.gamma * self.Q[s1, :].max()\n",
    "        \n",
    "        self.Q[s0, a0] = (1 - self.alpha)*self.Q[s0, a0] + self.alpha*q_upd        \n",
    "        self.eps.decrease()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 1000. Wins: 70, Loose: 59, Draw: 2240, Invalid Moves: 869, Eps: 0.6762000000000357\n",
      "Game 2000. Wins: 115, Loose: 81, Draw: 2445, Invalid Moves: 800, Eps: 0.33210000000007356\n",
      "Game 3000. Wins: 192, Loose: 103, Draw: 2711, Invalid Moves: 700, Eps: 0\n",
      "Game 4000. Wins: 284, Loose: 128, Draw: 2889, Invalid Moves: 584, Eps: 0\n",
      "Game 5000. Wins: 384, Loose: 95, Draw: 2862, Invalid Moves: 511, Eps: 0\n",
      "Game 6000. Wins: 479, Loose: 93, Draw: 2790, Invalid Moves: 421, Eps: 0\n",
      "Game 7000. Wins: 456, Loose: 107, Draw: 2909, Invalid Moves: 426, Eps: 0\n",
      "Game 8000. Wins: 653, Loose: 58, Draw: 2795, Invalid Moves: 274, Eps: 0\n",
      "Game 9000. Wins: 566, Loose: 70, Draw: 2829, Invalid Moves: 356, Eps: 0\n",
      "Game 10000. Wins: 834, Loose: 21, Draw: 2678, Invalid Moves: 135, Eps: 0\n",
      "Game 11000. Wins: 866, Loose: 16, Draw: 2682, Invalid Moves: 106, Eps: 0\n",
      "Game 12000. Wins: 875, Loose: 8, Draw: 2725, Invalid Moves: 102, Eps: 0\n",
      "Game 13000. Wins: 900, Loose: 9, Draw: 2673, Invalid Moves: 76, Eps: 0\n",
      "Game 14000. Wins: 920, Loose: 4, Draw: 2675, Invalid Moves: 54, Eps: 0\n",
      "Game 15000. Wins: 937, Loose: 5, Draw: 2625, Invalid Moves: 43, Eps: 0\n",
      "Game 16000. Wins: 954, Loose: 4, Draw: 2593, Invalid Moves: 36, Eps: 0\n",
      "Game 17000. Wins: 943, Loose: 2, Draw: 2625, Invalid Moves: 33, Eps: 0\n",
      "Game 18000. Wins: 949, Loose: 1, Draw: 2625, Invalid Moves: 35, Eps: 0\n",
      "Game 19000. Wins: 952, Loose: 0, Draw: 2646, Invalid Moves: 30, Eps: 0\n",
      "Game 20000. Wins: 974, Loose: 1, Draw: 2585, Invalid Moves: 12, Eps: 0\n",
      "Game 21000. Wins: 962, Loose: 0, Draw: 2614, Invalid Moves: 21, Eps: 0\n",
      "Game 22000. Wins: 969, Loose: 0, Draw: 2625, Invalid Moves: 15, Eps: 0\n",
      "Game 23000. Wins: 969, Loose: 0, Draw: 2609, Invalid Moves: 12, Eps: 0\n",
      "Game 24000. Wins: 975, Loose: 0, Draw: 2599, Invalid Moves: 14, Eps: 0\n",
      "Game 25000. Wins: 975, Loose: 0, Draw: 2617, Invalid Moves: 8, Eps: 0\n",
      "Game 26000. Wins: 980, Loose: 0, Draw: 2555, Invalid Moves: 10, Eps: 0\n",
      "Game 27000. Wins: 980, Loose: 0, Draw: 2575, Invalid Moves: 7, Eps: 0\n",
      "Game 28000. Wins: 986, Loose: 0, Draw: 2564, Invalid Moves: 4, Eps: 0\n",
      "Game 29000. Wins: 974, Loose: 0, Draw: 2603, Invalid Moves: 9, Eps: 0\n",
      "Game 30000. Wins: 980, Loose: 0, Draw: 2619, Invalid Moves: 7, Eps: 0\n",
      "Game 31000. Wins: 979, Loose: 0, Draw: 2600, Invalid Moves: 7, Eps: 0\n",
      "Game 32000. Wins: 984, Loose: 0, Draw: 2582, Invalid Moves: 4, Eps: 0\n",
      "Game 33000. Wins: 973, Loose: 0, Draw: 2636, Invalid Moves: 4, Eps: 0\n",
      "Game 34000. Wins: 986, Loose: 0, Draw: 2578, Invalid Moves: 4, Eps: 0\n",
      "Game 35000. Wins: 983, Loose: 0, Draw: 2580, Invalid Moves: 2, Eps: 0\n",
      "Game 36000. Wins: 978, Loose: 0, Draw: 2590, Invalid Moves: 5, Eps: 0\n",
      "Game 37000. Wins: 984, Loose: 0, Draw: 2585, Invalid Moves: 1, Eps: 0\n",
      "Game 38000. Wins: 979, Loose: 0, Draw: 2581, Invalid Moves: 5, Eps: 0\n",
      "Game 39000. Wins: 977, Loose: 0, Draw: 2588, Invalid Moves: 3, Eps: 0\n",
      "Game 40000. Wins: 968, Loose: 0, Draw: 2589, Invalid Moves: 10, Eps: 0\n",
      "Game 41000. Wins: 973, Loose: 0, Draw: 2597, Invalid Moves: 6, Eps: 0\n",
      "Game 42000. Wins: 977, Loose: 1, Draw: 2583, Invalid Moves: 6, Eps: 0\n",
      "Game 43000. Wins: 978, Loose: 1, Draw: 2544, Invalid Moves: 7, Eps: 0\n",
      "Game 44000. Wins: 988, Loose: 2, Draw: 2546, Invalid Moves: 3, Eps: 0\n",
      "Game 45000. Wins: 973, Loose: 0, Draw: 2635, Invalid Moves: 7, Eps: 0\n",
      "Game 46000. Wins: 978, Loose: 0, Draw: 2606, Invalid Moves: 5, Eps: 0\n",
      "Game 47000. Wins: 980, Loose: 0, Draw: 2620, Invalid Moves: 2, Eps: 0\n",
      "Game 48000. Wins: 978, Loose: 0, Draw: 2592, Invalid Moves: 2, Eps: 0\n",
      "Game 49000. Wins: 988, Loose: 0, Draw: 2568, Invalid Moves: 1, Eps: 0\n",
      "Game 50000. Wins: 983, Loose: 0, Draw: 2578, Invalid Moves: 0, Eps: 0\n"
     ]
    }
   ],
   "source": [
    "n_games = 50000\n",
    "\n",
    "env = make('tictactoe')\n",
    "trainer = QTrainer(env)\n",
    "agent = QClassicAgent()\n",
    "\n",
    "wins, loose, draw, invalid = 0, 0, 0, 0\n",
    "\n",
    "for game_no in range(n_games):\n",
    "    done = False\n",
    "    prev_state = trainer.reset()\n",
    "    while not done: \n",
    "        action = agent(prev_state)\n",
    "        next_state, reward, done = trainer.step(action)\n",
    "        \n",
    "        if reward == 1:\n",
    "            wins += 1\n",
    "        elif reward == -1:\n",
    "            loose += 1\n",
    "        elif reward == 0:\n",
    "            draw += 1\n",
    "        else:\n",
    "            invalid += 1\n",
    "            reward = -1\n",
    "\n",
    "        agent.learn(prev_state, action, next_state, reward, done)\n",
    "        prev_state = next_state\n",
    "\n",
    "    if game_no % 1000 == 999:\n",
    "        print(f'Game {game_no + 1}. Wins: {wins}, Loose: {loose}, Draw: {draw}, Invalid Moves: {invalid}, Eps: {agent.eps.value}')\n",
    "        wins, loose, draw, invalid = (0, 0, 0, 0)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
